{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bc2b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input path for delivery files\n",
    "delivery_path = \"abfss://ipl-incremental-data@incrementalipldata.dfs.core.windows.net/row/delivery/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff70fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for delivery data (optional but recommended)\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "\n",
    "delivery_schema = StructType([\n",
    "    StructField(\"match_id\", IntegerType(), True),\n",
    "    StructField(\"inning\", IntegerType(), True),\n",
    "    StructField(\"batting_team\", StringType(), True),\n",
    "    StructField(\"bowling_team\", StringType(), True),\n",
    "    StructField(\"over\", IntegerType(), True),\n",
    "    StructField(\"ball\", IntegerType(), True),\n",
    "    StructField(\"batsman\", StringType(), True),\n",
    "    StructField(\"non_striker\", StringType(), True),\n",
    "    StructField(\"bowler\", StringType(), True),\n",
    "    StructField(\"is_super_over\", IntegerType(), True),\n",
    "    StructField(\"wide_runs\", IntegerType(), True),\n",
    "    StructField(\"bye_runs\", IntegerType(), True),\n",
    "    StructField(\"legbye_runs\", IntegerType(), True),\n",
    "    StructField(\"noball_runs\", IntegerType(), True),\n",
    "    StructField(\"penalty_runs\", IntegerType(), True),\n",
    "    StructField(\"batsman_runs\", IntegerType(), True),\n",
    "    StructField(\"extra_runs\", IntegerType(), True),\n",
    "    StructField(\"total_runs\", IntegerType(), True),\n",
    "    StructField(\"player_dismissed\", StringType(), True),\n",
    "    StructField(\"dismissal_kind\", StringType(), True),\n",
    "    StructField(\"fielder\", StringType(), True)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecda9cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Autoloader to read new files in the /delivery/ folder\n",
    "df_delivery = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/mnt/delta/delivery/schema\")  # Keep track of the schema\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(delivery_schema)  # Apply schema\n",
    "    .load(delivery_path)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5224f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ingestion timestamp to the data\n",
    "df_delivery = df_delivery.withColumn(\"ingestion_ts\", current_timestamp())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e80bca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.connect.streaming.query.StreamingQuery at 0x7f6595e48aa0>"
     ]
    }
   ],
   "source": [
    "\n",
    "# Write the data into the `delivery_loading` Delta table incrementally\n",
    "(df_delivery.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", \"/mnt/delta/delivery/checkpoints\")  # Keep track of the processed files\n",
    "    .table(\"bronze.delivery_loading\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
