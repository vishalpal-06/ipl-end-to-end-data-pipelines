{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9071417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max timestamps from previous Extract_Max_ts task\n",
    "max_match_ts = dbutils.jobs.taskValues.get(\n",
    "    taskKey=\"Extract_Max_ts\", \n",
    "    key=\"max_match_ts\", \n",
    "    debugValue=\"2026-01-01 00:00:00\"\n",
    ")\n",
    "\n",
    "max_delivery_ts = dbutils.jobs.taskValues.get(\n",
    "    taskKey=\"Extract_Max_ts\", \n",
    "    key=\"max_delivery_ts\", \n",
    "    debugValue=\"2026-01-01 00:00:00\"\n",
    ")\n",
    "\n",
    "print(\"Max Match TS:\", max_match_ts)\n",
    "print(\"Max Delivery TS:\", max_delivery_ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1599301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df = spark.sql(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM bronze.match_loading\n",
    "    WHERE ingestion_ts <= TIMESTAMP('{max_match_ts}')\n",
    "\"\"\")\n",
    "\n",
    "archive_match_path = \"/Volumes/ipl_data_engineering/bronze/ipl_archive/match_backup/\"\n",
    "\n",
    "# Save as CSV\n",
    "(\n",
    "    match_df\n",
    "    .coalesce(1)  # optional: single file\n",
    "    .write\n",
    "    .mode(\"append\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(archive_match_path)\n",
    ")\n",
    "\n",
    "print(f\"Archived match_loading to {archive_match_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd0c1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_df = spark.sql(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM bronze.delivery_loading\n",
    "    WHERE ingestion_ts <= TIMESTAMP('{max_delivery_ts}')\n",
    "\"\"\")\n",
    "\n",
    "archive_delivery_path = \"/Volumes/ipl_data_engineering/bronze/ipl_archive/delivery_backup/\"\n",
    "\n",
    "(\n",
    "    delivery_df\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .mode(\"append\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(archive_delivery_path)\n",
    ")\n",
    "\n",
    "print(f\"Archived delivery_loading to {archive_delivery_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ca13a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "\n",
    "audit_data = [Row(\n",
    "    max_match_ts=max_match_ts,\n",
    "    max_delivery_ts=max_delivery_ts,\n",
    "    archived_at=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    ")]\n",
    "\n",
    "audit_df = spark.createDataFrame(audit_data)\n",
    "\n",
    "audit_path = \"/Volumes/ipl_data_engineering/bronze/ipl_archive/max_ts_snapshot/\"\n",
    "\n",
    "(\n",
    "    audit_df\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .mode(\"append\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(audit_path)\n",
    ")\n",
    "\n",
    "print(f\"Saved audit snapshot to {audit_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
